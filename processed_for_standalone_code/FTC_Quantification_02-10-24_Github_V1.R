#Code to process site NEON temperature probe data and quantify freeze-thaw cycling using FTCQuant
#Code generated by Angela Possinger and Erin Rooney, August 2020
#Updated for FTC across all sites, June 2023
#Updated for JGRB submission repository, February 2024

#Packages----------------------------------
library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
library(reshape)
library(reshape2)
library(plyr)
library(Rmisc)
library(devtools)
install_github("BajczA475/FTCQuant/FTCQuant")
library(FTCQuant)
library(data.table)
library(googledrive)

#Load soil temp data as NEON data products####

#These data files are generated by NEON data import code within repository

temp_fall1 = readRDS("/.../temp_fall1.RData")
temp_fall2 = readRDS("/.../temp_fall2.RData")

temp_winter1 = readRDS("/.../temp_winter1.RData")
temp_winter2 = readRDS("/.../temp_winter2.RData")

temp_spring1 = readRDS("/.../temp_spring1.RData")
temp_spring2 = readRDS("/.../temp_spring2.RData")

temp_summer1 = readRDS("/.../temp_summer1.RData")
temp_summer2 = readRDS("/.../temp_summer2.RData")

#Working with NEON data############################
#Each data product has 4 elements
#Example:
#readme_00041 #A large readme file giving information about data collection and format
#sensor_positions_00041 #Gives locations and depths of probes
#ST_30_minute #The actual time series temperature data
#variables_00041 #Variable definitions

#Temperature and sensor depths####

#In list format (for FTC function)
temp.dat.fall1 = list(fall1 = temp_fall1$ST_30_minute) 
temp.dat.fall2 = list(fall2 = temp_fall2$ST_30_minute) 
temp.dat.winter1 = list(winter1 = temp_winter1$ST_30_minute) 
temp.dat.winter2 = list(winter2 = temp_winter2$ST_30_minute) 
temp.dat.spring1 = list(spring1 = temp_spring1$ST_30_minute) 
temp.dat.spring2 = list(spring2 = temp_spring2$ST_30_minute) 
temp.dat.summer1 = list(summer1 = temp_summer1$ST_30_minute) 
temp.dat.summer2 = list(summer2 = temp_summer2$ST_30_minute)

pos.dat.fall1 = list(fall1 = temp_fall1$sensor_positions_00041) 
pos.dat.fall2 = list(fall2 = temp_fall2$sensor_positions_00041) 
pos.dat.winter1 = list(winter1 = temp_winter1$sensor_positions_00041) 
pos.dat.winter2 = list(winter2 = temp_winter2$sensor_positions_00041) 
pos.dat.spring1 = list(spring1 = temp_spring1$sensor_positions_00041) 
pos.dat.spring2 = list(spring2 = temp_spring2$sensor_positions_00041) 
pos.dat.summer1 = list(summer1 = temp_summer1$sensor_positions_00041) 
pos.dat.summer2 = list(summer2 = temp_summer2$sensor_positions_00041)

#As data frames (for sensor plots)
temp.dat.fall1.df = temp_fall1$ST_30_minute
temp.dat.fall2.df = temp_fall2$ST_30_minute
temp.dat.winter1.df = temp_winter1$ST_30_minute
temp.dat.winter2.df = temp_winter2$ST_30_minute
temp.dat.spring1.df = temp_spring1$ST_30_minute
temp.dat.spring2.df = temp_spring2$ST_30_minute
temp.dat.summer1.df = temp_summer1$ST_30_minute
temp.dat.summer2.df = temp_summer2$ST_30_minute

#Sensor position dat as data frames (for sensor plots)
pos.dat.fall1.df = temp_fall1$sensor_positions_00041
pos.dat.fall2.df = temp_fall2$sensor_positions_00041
pos.dat.winter1.df = temp_winter1$sensor_positions_00041
pos.dat.winter2.df = temp_winter2$sensor_positions_00041
pos.dat.spring1.df = temp_spring1$sensor_positions_00041
pos.dat.spring2.df = temp_spring2$sensor_positions_00041
pos.dat.summer1.df = temp_summer1$sensor_positions_00041
pos.dat.summer2.df = temp_summer2$sensor_positions_00041

#Sensor data plots

#1: Single site, sensor, depth####

# #Example - fall 1
temp_fall1_data = temp_fall1$ST_30_minute
temp_fall1_sensor = temp_fall1$sensor_positions_00041

#1a: Healy (AK)####
HEAL_501_001_fall = subset(temp_fall1_data, siteID == "HEAL" & verticalPosition == "501" & horizontalPosition == "001") 

pdf("/.../Dataset/Updated Processed Data/NEON Sensor Test Plots/HEAL_501_001_fall.pdf")
print(ggplot(HEAL_501_001_fall) + geom_line(aes(x=startDateTime, y=soilTempMean))) 
dev.off() 

#1b: Bartlett Experimental Forest (NH)####
BART_501_001_fall = subset(temp_fall1_data, siteID == "BART" & verticalPosition == "501" & horizontalPosition == "001") 

pdf("/.../Dataset/Updated Processed Data/NEON Sensor Test Plots/BART_501_001_fall.pdf")
print(ggplot(BART_501_001_fall) + geom_line(aes(x=startDateTime, y=soilTempMean))) 
dev.off() 

#1c: ABBY (PNW)####
ABBY_501_001_fall = subset(temp_fall1_data, siteID == "ABBY" & verticalPosition == "501" & horizontalPosition == "001") 

pdf("/.../Dataset/Updated Processed Data/NEON Sensor Test Plots/ABBY_501_001_fall.pdf")
print(ggplot(ABBY_501_001_fall) + geom_line(aes(x=startDateTime, y=soilTempMean))) 
dev.off() 

#1d: JORN####
#Warm and dry sites with high FTC at surface

JORN_501_001_fall = subset(temp.dat.fall1.df, siteID == "JORN" & verticalPosition == "501" & horizontalPosition == "001") 

pdf("/.../Dataset/Updated Processed Data/NEON Sensor Test Plots/JORN_501_001_fall1.pdf")
print(ggplot(JORN_501_001_fall) + geom_line(aes(x=startDateTime, y=soilTempMean))) 
dev.off() 

JORN_501_001_winter = subset(temp.dat.winter1.df, siteID == "JORN" & verticalPosition == "501" & horizontalPosition == "001") 

pdf("/.../Dataset/Updated Processed Data/NEON Sensor Test Plots/JORN_501_001_winter1.pdf")
print(ggplot(JORN_501_001_winter) + geom_line(aes(x=startDateTime, y=soilTempMean))) 
dev.off() 

#MOAB, CPER, STER 

#2: All sites####
season_list = list(fall1 = temp.dat.fall1.df, fall2 = temp.dat.fall2.df, winter1 = temp.dat.winter1.df, winter2 = temp.dat.winter2.df, spring1 = temp.dat.spring1.df, spring2 = temp.dat.spring2.df, summer1 = temp.dat.summer1.df, summer2 = temp.dat.summer2.df)

sites = c("ABBY", "BARR", "BART", "BLAN", "BONA", "CLBJ", "CPER", "DCFS", "DEJU", "DELA", "DSNY", "GRSM", "HARV", "HEAL", "JORN", "KONA", "KONZ", "LENO", "MLBS", "MOAB", "NOGP", "NIWO", "OAES", "ONAQ", "ORNL", "OSBS", "RMNP", "SCBI", "SERC", "SJER", "SOAP", "SRER", "STEI", "STER", "TALL", "TOOL", "TREE", "UNDE", "WOOD", "WREF")

fall1_plot = function(site){
  #site = siteID
  
  dat = season_list$fall1
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_fall1-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (09-01-2018 to 11-30-2018)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Fall1-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  fall1_plot(site=sites[i])
}

fall2_plot = function(site){
  #site = siteID
  
  dat = season_list$fall2
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_fall2-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (09-01-2019 to 11-30-2019)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Fall2-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  fall2_plot(site=sites[i])
}

winter1_plot = function(site){
  #site = siteID
  
  dat = season_list$winter1
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_winter1-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (12-01-2018 to 02-28-2019)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Winter1-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  winter1_plot(site=sites[i])
}

winter2_plot = function(site){
  #site = siteID
  
  dat = season_list$winter2
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_winter2-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (12-01-2019 to 02-29-2020)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Winter2-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  winter2_plot(site=sites[i])
}

spring1_plot = function(site){
  #site = siteID
  
  dat = season_list$spring1
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_spring1-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (03-01-2019 to 05-30-2019)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Spring1-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  spring1_plot(site=sites[i])
}

spring2_plot = function(site){
  #site = siteID
  
  dat = season_list$spring2
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_spring2-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (03-01-2020 to 05-30-2020)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Spring2-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  spring2_plot(site=sites[i])
}

summer1_plot = function(site){
  #site = siteID
  
  dat = season_list$summer1
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_summer1-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (06-01-2019 to 08-31-2019)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Summer1-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  summer1_plot(site=sites[i])
}

summer2_plot = function(site){
  #site = siteID
  
  dat = season_list$summer2
  
  dat_site = subset(dat, siteID == site)
  
  setwd("/.../Dataset/Updated Processed Data/NEON Sensor All Plots/")
  
  pdf(paste0("sensor_dat_summer2-",site,".pdf"), width=14, height=14)
  
  print(ggplot(dat_site) + 
          geom_line(aes(x=startDateTime, y=soilTempMean)) +
          facet_grid(verticalPosition~horizontalPosition) + 
          labs(x="Date (06-01-2020 to 08-31-2020)", y=expression(paste("Soil temperature (",degree,"C)"))) + 
          geom_hline(yintercept=0, linetype="longdash") + 
          ggtitle(paste0("Summer2-",site)) + 
          theme_bw() + 
          theme(panel.grid=element_blank()))
  
  
  dev.off() 
  
}

for (i in 1:length(sites)){
  summer2_plot(site=sites[i])
}

#Check FTC function####
#dur.vec = 8 is 4 hours (8 half hour timesteps)
#thres.vec = upper limit (1.5 C)
#mag.vec = plus or minus (3 C)

BART_501_001_fall_test = list(BART = as.data.frame(BART_501_001_fall[,c("startDateTime","soilTempMean")]))
FTC_BART_analysis = freeze.thaw.analysis(BART_501_001_fall_test, mag.vec=3, dur.vec=8, thres.vec=1.5) #Def1 should be 0
# $data
# Def1
# 1    0
# 
# $Definition.key
# Definition Duration Min_Offset Temp_Threshold
# 1       Def1        8          3            1.5

#Freeze-thaw analysis: #########################

#FTC for 4 degree mag.vec (+/ 2 degrees), 12 hour duration (24 timesteps)####

#Fall 1########
FTC_fall1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                    y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                    name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                    year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  #Note: NGPR in SOMMOS dataset is NOGP in siteID
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC=freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep="")) 
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.fall1)){
  cur.name=names(temp.dat.fall1)[i]
  FTC_fall1(x=temp.dat.fall1[[i]], 
           y=pos.dat.fall1[[i]], 
           name=cur.name,
           year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Fall 2########

FTC_fall2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                     y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                     name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                     year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff). Function also fails if the first row is NA. 
  #Replaced columns with NA > 3% of timepoints or that start with NA with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0

  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep="")) 
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.fall2)){
  cur.name=names(temp.dat.fall2)[i]
  FTC_fall2(x=temp.dat.fall2[[i]], 
            y=pos.dat.fall2[[i]], 
            name=cur.name,
            year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Winter 1########

FTC_winter1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                     y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                     name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                     year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC. Elements excluded that start with NAs. 
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep="")) 
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.winter1)){
  cur.name=names(temp.dat.winter1)[i]
  FTC_winter1(x=temp.dat.winter1[[i]], 
            y=pos.dat.winter1[[i]], 
            name=cur.name,
            year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Winter 2########
FTC_winter2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep="")) 
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.winter2)){
  cur.name=names(temp.dat.winter2)[i]
  FTC_winter2(x=temp.dat.winter2[[i]], 
              y=pos.dat.winter2[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Spring 1########
FTC_spring1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep="")) 
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.spring1)){
  cur.name=names(temp.dat.spring1)[i]
  FTC_spring1(x=temp.dat.spring1[[i]], 
              y=pos.dat.spring1[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Spring 2########
FTC_spring2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.spring2)){
  cur.name=names(temp.dat.spring2)[i]
  FTC_spring2(x=temp.dat.spring2[[i]], 
              y=pos.dat.spring2[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Summer 1########
FTC_summer1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.summer1)){
  cur.name=names(temp.dat.summer1)[i]
  FTC_summer1(x=temp.dat.summer1[[i]], 
              y=pos.dat.summer1[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Summer 2########
FTC_summer2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=4, dur.vec=24, thres.vec=2)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")
  
  write.csv(FTC.all, paste0("FTC_12_2-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.summer2)){
  cur.name=names(temp.dat.summer2)[i]
  FTC_summer2(x=temp.dat.summer2[[i]], 
              y=pos.dat.summer2[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Create final dataset########################################
setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_12_2/")

FTC_12_2 = list.files(pattern="*.csv")

myfiles = lapply(FTC_12_2, read.csv)

combined_FTC = do.call(rbind, myfiles)

combined_FTC$mag.vec.degC = rep("4",nrow(combined_FTC))

combined_FTC$dur.vec.hr = rep("12",nrow(combined_FTC))

combined_FTC$thres.vec.degC = rep("2",nrow(combined_FTC))

combined_dat = combined_FTC

#Convert season to character and drop the last digit (this shouldn't happen in the function - why? For long term fixing...)
combined_dat$season=as.character(combined_dat$season)
combined_dat$season = substr(combined_dat$season, 1 , nchar(combined_dat$season)-1)

#Back to factor....
combined_dat$season=as.factor(combined_dat$season)

final_dat=combined_dat[,-1]

#Manually excluded sites####
#Make new column for column_names + season and year 

final_dat$sensor_ID = paste0(final_dat$column_names,"_",final_dat$season,"_",final_dat$year)

#All Summer 2 (incomplete data)
final_dat_filtered = final_dat %>% filter(!(season == "summer" & year = 2))

#Individual sensors excluded (see notes on Google Doc) (17)
#DEJU_508_001_fall_1
#NIWO_508_001_fall_2
#JORN_509_004_spring_1
#MLBS_507_001_spring_2 
#MLBS_507_004_spring_2 
#ONAQ_504_004_spring_2 
#WREF_506_001_spring_2
#DELA_506_002_summer_1
#TREE_506_002_summer_1
#DEJU_508_001_winter_1
#KONZ_501_002_winter_1
#NIWO_508_001_winter_1
#CLBJ_508_001_winter_2
#KONZ_501_002_winter_2
#MLBS_507_004_winter_2
#MOAB_508_002_winter_2
#RMNP_501_004_winter_2

final_dat_filtered_2 = final_dat_filtered %>% filter(!(sensor_ID == "DEJU_508_001_fall_1" | sensor_ID == "NIWO_508_001_fall_2" | sensor_ID == "JORN_509_004_spring_1" | sensor_ID == "MLBS_507_001_spring_2" | sensor_ID == "MLBS_507_004_spring_2" | sensor_ID == "ONAQ_504_004_spring_2" | sensor_ID == "WREF_506_001_spring_2" | sensor_ID == "DELA_506_002_summer_1" | sensor_ID == "TREE_506_002_summer_1" | sensor_ID == "DEJU_508_001_winter_1" | sensor_ID == "KONZ_501_002_winter_1" | sensor_ID == "NIWO_508_001_winter_1" | sensor_ID == "CLBJ_508_001_winter_2" | sensor_ID == "KONZ_501_002_winter_2" | sensor_ID == "MLBS_507_004_winter_2" | sensor_ID == "MOAB_508_002_winter_2" | sensor_ID == "RMNP_501_004_winter_2"))

#12_2 final FTC product##########
write.csv(final_dat_filtered_2, "/.../Dataset/Updated Processed Data/Processed Sensor Data/final_dat_12hr_2mag_06-20-23.csv")
saveRDS(final_dat_filtered_2, file = "/.../Dataset/Updated Processed Data/Processed Sensor Data/final_dat_12hr_2mag_06-20-23.RData")

#FTC for 3 degree mag.vec (+/ 1.5 degrees), 4 hour duration (8 timesteps)#######################
#Fall 1########
FTC_fall1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                     y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                     name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                     year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC=freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.fall1)){
  cur.name=names(temp.dat.fall1)[i]
  FTC_fall1(x=temp.dat.fall1[[i]], 
            y=pos.dat.fall1[[i]], 
            name=cur.name,
            year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Fall 2########
FTC_fall2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                     y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                     name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                     year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC=freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.fall2)){
  cur.name=names(temp.dat.fall2)[i]
  FTC_fall2(x=temp.dat.fall2[[i]], 
            y=pos.dat.fall2[[i]], 
            name=cur.name,
            year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Winter 1########
FTC_winter1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=2, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.winter1)){
  cur.name=names(temp.dat.winter1)[i]
  FTC_winter1(x=temp.dat.winter1[[i]], 
              y=pos.dat.winter1[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Winter 2########
FTC_winter2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.winter2)){
  cur.name=names(temp.dat.winter2)[i]
  FTC_winter2(x=temp.dat.winter2[[i]], 
              y=pos.dat.winter2[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Spring 1########
FTC_spring1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.spring1)){
  cur.name=names(temp.dat.spring1)[i]
  FTC_spring1(x=temp.dat.spring1[[i]], 
              y=pos.dat.spring1[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Spring 2########
FTC_spring2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.spring2)){
  cur.name=names(temp.dat.spring2)[i]
  FTC_spring2(x=temp.dat.spring2[[i]], 
              y=pos.dat.spring2[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Summer 1########
FTC_summer1 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.summer1)){
  cur.name=names(temp.dat.summer1)[i]
  FTC_summer1(x=temp.dat.summer1[[i]], 
              y=pos.dat.summer1[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Summer 2########
FTC_summer2 = function(x, #list of dataframes (element ST_30_minute from NEON data product)
                       y, #list of dataframes (element sensor_positions_00041 from NEON data product)
                       name, #names of elements in x and y (vector). Names must end in a single digit number (year) in this case. 0 = NA. 
                       year #last character of names, which defines the year of measurements (in this case 1 or 2). 0 = NA. 
) {
  
  #Reduce dataframe to relevant columns (speeds up dcast)
  reduced_dat <- x[,c("siteID","verticalPosition","horizontalPosition","startDateTime","soilTempMean")]
  
  reduced_dat = subset(reduced_dat, siteID == "ABBY" | siteID == "BARR" | siteID == "BART" | siteID == "BLAN" | siteID == "BONA" | siteID == "CLBJ" | siteID == "CPER" | siteID == "DCFS" | siteID == "DEJU" | siteID == "DELA" | siteID == "DSNY" | siteID == "GRSM" | siteID == "HARV" | siteID == "HEAL" | siteID == "JORN" | siteID == "KONA" | siteID == "KONZ" | siteID == "LENO" | siteID == "MLBS" | siteID == "MOAB" | siteID == "NOGP" | siteID == "NIWO" | siteID == "OAES" | siteID == "ONAQ" | siteID == "ORNL" | siteID == "OSBS" | siteID == "RMNP" | siteID == "SCBI" | siteID == "SERC" | siteID == "SJER" | siteID == "SOAP" | siteID == "SRER" | siteID == "STEI" | siteID == "STER" | siteID == "TALL" | siteID == "TOOL" | siteID == "TREE" | siteID == "UNDE" | siteID == "WOOD" | siteID == "WREF")
  
  #Convert vertical, horizontal, and siteID to factors
  reduced_dat$verticalPosition=as.factor(reduced_dat$verticalPosition)
  reduced_dat$horizontalPosition=as.factor(reduced_dat$horizontalPosition)
  reduced_dat$siteID=as.factor(reduced_dat$siteID)
  
  #Reformat from "long" to "wide" form. Now the first column is the date-time
  #the other columns all correspond to one individual unit (probe), i.e. 1 site, 1 core, 1 depth 
  reduced_dat_Wide <- reshape2::dcast(reduced_dat, startDateTime ~ siteID + verticalPosition + horizontalPosition)
  
  #Freeze.thaw.analysis function fails with too many missing values (they do not provide a cutoff) 
  #For now, replacing columns with NA > 3% of timepoints with 0s (should be a flat line)
  #Most probes with missing data >3% are missing most if not all measurements
  
  #How many NAs is 3%?
  Max_NA <- nrow(reduced_dat_Wide)*0.03
  
  #Count # of NAs in each column
  NA_count <- as.data.frame(sapply(reduced_dat_Wide, function(x) sum(is.na(x))))
  colnames(NA_count) <- c("NA_count")
  
  #Converts all columns with names that match rows in NA_count with NA > 3% to 0s 
  reduced_dat_Wide[,c(rownames(NA_count)[NA_count$NA_count >= Max_NA])] <- 0
  
  #Converts all columns that start with NA to 0 
  #Reduced to first column only 
  first_col = reduced_dat_Wide[1,]
  names = colnames(first_col)[colSums(is.na(first_col)) > 0]
  reduced_dat_Wide[,c(names)] <- 0
  
  #Convert date to character
  reduced_dat_Wide$startDateTime=as.character.Date(reduced_dat_Wide$startDateTime)
  names(reduced_dat_Wide)[names(reduced_dat_Wide) == 'startDateTime'] <- 'date'
  
  # #Dimensions of final dataframe
  # dim(reduced_dat_Wide)
  
  #Freeze-thaw analysis ####################################
  #Names of all columns but date, which will become the names of each element in the data.list for freeze thaw function
  column_names=as.vector(colnames(reduced_dat_Wide[,2:ncol(reduced_dat_Wide)]))
  #Number of columns to use in the data.list 
  column_number=as.vector(2:ncol(reduced_dat_Wide))
  
  #This function makes a list of elements, extracting row 1 (date) and then sequentially each column (y)
  #Each date and data column is a new element in the list, named by the list of column names above (x) 
  fun1 <- function(name,number) {
    data.list <- list(name = reduced_dat_Wide[,c(1,number)])
  }
  
  #This applies the function to loop through the list of column names (name) and numbers (number) 
  data.list = mapply(fun1,column_names,column_number)
  
  #The actual analysis function: this is where the parameters can be changed. 
  #mag.vec = degrees above or below thres.vec (0 degrees C) to induce FTC
  #dur.vec = duration of time (number of timesteps) above or below mag.vec to induce FTC
  FTC = freeze.thaw.analysis(data.list, mag.vec=3, dur.vec=8, thres.vec=1.5)
  
  #Combine FTC data output (Def1) with column_names 
  FTC.dat = cbind(FTC$data, column_names)
  colnames(FTC.dat)=c("Def1","column_names")
  
  #Measurements to exclude?
  Exclude_1 = as.data.frame(rownames(NA_count)[NA_count$NA_count >= Max_NA])
  colnames(Exclude_1) = c("column_names")
  Exclude_2 = as.data.frame(names)
  colnames(Exclude_2) = c("column_names")
  Exclude = rbind(Exclude_1, Exclude_2)
  
  Exclude$Label = rep("Exclude", length(Exclude))
  colnames(Exclude)=c("column_names","Label")
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/Excluded Sensors")
  write.csv(Exclude, paste0("Excluded-",name, ".csv", sep=""))
  
  FTC.full <- Exclude %>% right_join(FTC.dat, by=c("column_names"))
  FTC.full$Label[is.na(FTC.full$Label)] = "Keep"
  
  FTC.full$Def1[FTC.full$Label == "Exclude"] <- NA
  FTC.full$season=rep(name,nrow(FTC.full))
  FTC.full$year=rep(year,nrow(FTC.full))
  
  #Split column to create factors (depth and core)
  FTC.full$column_names_2=FTC.full$column_names
  
  #Final FTC data
  FTC.full.final = FTC.full %>% separate(column_names_2, c("site","depth","core"), sep="_")
  
  #Core depths###################
  Sensor_Pos = y
  Sensor_Pos$HOR.VER.2=Sensor_Pos$HOR.VER
  Sensor_Pos = Sensor_Pos %>% separate(HOR.VER.2, c("HOR", "VER"))  
  
  Sensor_Pos=Sensor_Pos[,c("siteID","HOR","VER","zOffset")]
  #220 rows
  
  #Remove duplicate rows? This works in this example, but it's not generalizable. 
  #Healy has two zOffsets. Negative values seem to be correct depths, keeping only first row of HEAL
  Sensor_Pos_Distinct = distinct_at(Sensor_Pos, vars(siteID, HOR, VER), .keep_all = TRUE)
  
  #Combine names for ID column
  Sensor_Pos_Distinct = as.data.frame(unite(Sensor_Pos_Distinct, "column_names", c(siteID, VER, HOR), sep = "_", remove = FALSE))
  
  colnames(Sensor_Pos_Distinct)=c("column_names","site_pos","core_pos","depth_pos","depth_m")
  
  #Combine with FTC calculations
  FTC.all = Sensor_Pos_Distinct %>% right_join(FTC.full.final, by=c("column_names"))
  
  setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")
  
  write.csv(FTC.all, paste0("FTC_4_1.5-",name, ".csv", sep="")) #Change append to name to make parameters in FTC function
  
}

for (i in 1:length(pos.dat.summer2)){
  cur.name=names(temp.dat.summer2)[i]
  FTC_summer2(x=temp.dat.summer2[[i]], 
              y=pos.dat.summer2[[i]], 
              name=cur.name,
              year=substr(cur.name, nchar(cur.name),nchar(cur.name)))
}

#Create final dataset########################################
setwd("/.../Dataset/Updated Processed Data/Processed Sensor Data/FTC_dat/FTC_4_1.5/")

FTC_4_1.5 = list.files(pattern="*.csv")

myfiles = lapply(FTC_4_1.5, read.csv)

combined_FTC_4 = do.call(rbind, myfiles)

combined_FTC_4$mag.vec.degC = rep("3",nrow(combined_FTC_4))

combined_FTC_4$dur.vec.hr = rep("4",nrow(combined_FTC_4))

combined_FTC_4$thres.vec.degC = rep("1.5",nrow(combined_FTC_4))

combined_dat_4 = combined_FTC_4

#Convert season to character and drop the last digit (this shouldn't happen in the function - why? For long term fixing...)
combined_dat_4$season=as.character(combined_dat_4$season)
combined_dat_4$season = substr(combined_dat_4$season, 1 , nchar(combined_dat_4$season)-1)

#Back to factor....
combined_dat_4$season=as.factor(combined_dat_4$season)

final_dat_4 = combined_dat_4[,-1]

#Manually excluded sites####
#Make new column for column_names + season and year 

final_dat_4$sensor_ID = paste0(final_dat_4$column_names,"_",final_dat_4$season,"_",final_dat_4$year)

#All Summer 2 (incomplete data)
final_dat_4_filtered = final_dat_4 %>% filter(!(season == "summer" & year == 2))

#Individual sensors excluded (see notes on Google Doc) (17)
#DEJU_508_001_fall_1
#NIWO_508_001_fall_2
#JORN_509_004_spring_1
#MLBS_507_001_spring_2 
#MLBS_507_004_spring_2 
#ONAQ_504_004_spring_2 
#WREF_506_001_spring_2
#DELA_506_002_summer_1
#TREE_506_002_summer_1
#DEJU_508_001_winter_1
#KONZ_501_002_winter_1
#NIWO_508_001_winter_1
#CLBJ_508_001_winter_2
#KONZ_501_002_winter_2
#MLBS_507_004_winter_2
#MOAB_508_002_winter_2
#RMNP_501_004_winter_2

final_dat_4_filtered_2 = final_dat_4_filtered %>% filter(!(sensor_ID == "DEJU_508_001_fall_1" | sensor_ID == "NIWO_508_001_fall_2" | sensor_ID == "JORN_509_004_spring_1" | sensor_ID == "MLBS_507_001_spring_2" | sensor_ID == "MLBS_507_004_spring_2" | sensor_ID == "ONAQ_504_004_spring_2" | sensor_ID == "WREF_506_001_spring_2" | sensor_ID == "DELA_506_002_summer_1" | sensor_ID == "TREE_506_002_summer_1" | sensor_ID == "DEJU_508_001_winter_1" | sensor_ID == "KONZ_501_002_winter_1" | sensor_ID == "NIWO_508_001_winter_1" | sensor_ID == "CLBJ_508_001_winter_2" | sensor_ID == "KONZ_501_002_winter_2" | sensor_ID == "MLBS_507_004_winter_2" | sensor_ID == "MOAB_508_002_winter_2" | sensor_ID == "RMNP_501_004_winter_2"))

#4_1.5 final FTC product#########
write.csv(final_dat_4_filtered_2, "/.../Dataset/Updated Processed Data/Processed Sensor Data/final_dat_4hr_1.5mag_06-20-23.csv")
saveRDS(final_dat_4_filtered_2, file = "/.../Dataset/Updated Processed Data/Processed Sensor Data/final_dat_4hr_1.5mag_06-20-23.RData")

#Overview of soil temperature patterns for selected sites####
#CPER (grassland?)
#TOOL (tundra)
#BARR (boreal forest)
#BART (temperate forest, colder)
#SERC (temperate forest, warmer)
#OSBS (warm and wet)

#Seasons
Fall = dplyr::filter(temp.dat.fall2.df, siteID == "CPER" | siteID == "OSBS" | siteID == "BARR" | siteID == "BONA" | siteID == "BART" | siteID == "SCBI")
Winter = dplyr::filter(temp.dat.winter2.df, siteID == "CPER" | siteID == "OSBS" | siteID == "BARR" | siteID == "BONA" | siteID == "BART" | siteID == "SCBI")
Spring = dplyr::filter(temp.dat.spring1.df, siteID == "CPER" | siteID == "OSBS" | siteID == "BARR" | siteID == "BONA" | siteID == "BART" | siteID == "SCBI")
Summer = dplyr::filter(temp.dat.summer1.df, siteID == "CPER" | siteID == "OSBS" | siteID == "BARR" | siteID == "BONA" | siteID == "BART" | siteID == "SCBI")

#ID by season
Fall$season = rep("Fall", nrow(Fall))
Winter$season = rep("Winter", nrow(Winter))
Spring$season = rep("Spring", nrow(Spring))
Summer$season = rep("Summer", nrow(Summer))

#Combined
year_dat = rbind(Fall, Winter, Spring, Summer)

#Mean soil temp
year_dat = year_dat %>% 
  dplyr::group_by(siteID, season, verticalPosition, startDateTime) %>% 
  dplyr::summarize(mean_temp = mean(soilTempMean, na.rm=TRUE), sd_temp = sd(soilTempMean, na.rm=TRUE), n = n())

year_dat$climate_group = ifelse(year_dat$siteID == "BARR" | year_dat$siteID == "BONA", "Cold and dry","temp")
year_dat$climate_group = ifelse(year_dat$siteID == "BART" | year_dat$siteID == "SCBI" | year_dat$siteID == "OSBS", "Warm and wet", year_dat$climate_group)
year_dat$climate_group = ifelse(year_dat$climate_group == "temp", "Warm and dry", year_dat$climate_group) 
                                
year_dat$season = factor(year_dat$season, levels=c("Spring","Summer","Fall","Winter"))
year_dat$site_ID = factor(year_dat$siteID, levels=c("BARR","BONA","BART","SCBI","OSBS","CPER"))

pdf("/.../Updated Figures/Updated Component Figures/sensor_plots_surface.pdf", width=13, height=9)
print(ggplot(subset(year_dat, verticalPosition == "502")) + 
        geom_ribbon(aes(x=startDateTime, ymin=mean_temp-sd_temp, ymax=mean_temp+sd_temp), fill="grey90") + 
        geom_line(aes(x=startDateTime, y=mean_temp, color=climate_group), size=0.5) + 
        scale_color_manual(values=c("#5F9EA0","#D38E17","#4B8E17")) + 
        labs(color="Climate group", x="Date", y=expression(paste("Soil temperature (",degree,"C)"))) + 
        facet_grid(site_ID~season, scales="free_x") + 
        geom_hline(yintercept=0, linetype="longdash") + 
        theme_bw() + 
        theme(legend.text=element_text(size=12,color="black"), 
              axis.text=element_text(size=12, color="black"),
              axis.title=element_text(size=12, color="black"),
              panel.grid=element_blank(), 
              strip.text=element_text(size=12, color="black"),
              legend.title=element_text(size=12, color="black")))
dev.off() 

#CPER winter detail 

#CPER 
CPER = dplyr::filter(year_dat, siteID == "CPER" & season == "Winter" & verticalPosition == "502")

CPER_timesub = dplyr::filter(CPER, startDateTime > "2020-01-01" & startDateTime < "2020-01-10")

pdf("/.../Updated Figures/Updated Component Figures/CPER_winter.pdf", width=10, height=8)
print(ggplot(CPER_timesub) + 
        geom_ribbon(aes(x=startDateTime, ymin=mean_temp-sd_temp, ymax=mean_temp+sd_temp), fill="grey90") + 
        geom_line(aes(x=startDateTime, y=mean_temp), color="#D38E17", size=0.5) + 
        labs(x="Date", y=expression(paste("Soil temperature (",degree,"C)"))) + 
        #facet_nested(site_ID~season, scales="free_x") + 
        geom_hline(yintercept=0, linetype="longdash") + 
        theme_bw() + 
        theme(legend.text=element_text(size=12,color="black"), 
              axis.text=element_text(size=12, color="black"),
              panel.grid=element_blank(), 
              legend.title=element_text(size=12, color="black")))
dev.off() 


